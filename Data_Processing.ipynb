{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKWAuSRy7jtBjRDkzv5OlG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swastik200/30DaysOfDSA/blob/main/Data_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W0CvjWF2SNzc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kj3-20_Sph6",
        "outputId": "1bae78b3-2dac-4df1-a34f-0bd8e1c3ce52"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/CMAPSS.zip\" \"/content\"\n",
        "!unzip CMAPSS.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo7NNb9RVMvG",
        "outputId": "ad723030-f975-4789-9cb2-60ceecc35e60"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  CMAPSS.zip\n",
            "  inflating: CMAPSS/readme.txt       \n",
            "  inflating: CMAPSS/RUL_FD001.txt    \n",
            "  inflating: CMAPSS/RUL_FD002.txt    \n",
            "  inflating: CMAPSS/RUL_FD003.txt    \n",
            "  inflating: CMAPSS/RUL_FD004.txt    \n",
            "  inflating: CMAPSS/test_FD001.txt   \n",
            "  inflating: CMAPSS/test_FD002.txt   \n",
            "  inflating: CMAPSS/test_FD003.txt   \n",
            "  inflating: CMAPSS/test_FD004.txt   \n",
            "  inflating: CMAPSS/train_FD001.txt  \n",
            "  inflating: CMAPSS/train_FD002.txt  \n",
            "  inflating: CMAPSS/train_FD003.txt  \n",
            "  inflating: CMAPSS/train_FD004.txt  \n",
            "  inflating: CMAPSS/x.txt            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAXLIFE = 120\n",
        "SCALE = 1\n",
        "RESCALE = 1\n",
        "true_rul = []\n",
        "test_engine_id = 0\n",
        "training_engine_id = 0"
      ],
      "metadata": {
        "id": "xEFL95FEVai-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kink_RUL(cycle_list, max_cycle):\n",
        "    '''\n",
        "    Piecewise linear function with zero gradient and unit gradient\n",
        "\n",
        "            ^\n",
        "            |\n",
        "    MAXLIFE |-----------\n",
        "            |            \\\n",
        "            |             \\\n",
        "            |              \\\n",
        "            |               \\\n",
        "            |                \\\n",
        "            |----------------------->\n",
        "    '''"
      ],
      "metadata": {
        "id": "h7v8HiiIVdXw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cycles = np.array([10, 20, 30, 40, 50])"
      ],
      "metadata": {
        "id": "W-802bHkVjiV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_cycle = np.max(cycles)"
      ],
      "metadata": {
        "id": "rM8FNEPkV7kY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kink_RUL = []\n",
        "\n",
        "for i in range(0, len(cycle_list)):\n",
        "    if i == knee_point:  # Check if the current index is the knee point\n",
        "        kink_RUL.append(MAXLIFE)\n",
        "    else:\n",
        "        tmp = MAXLIFE * (stable_life / (max_cycle - knee_point))\n",
        "        kink_RUL.append(tmp)\n",
        "\n",
        "# Make sure to return the kink_RUL list inside a function\n",
        "def kink_RUL():\n",
        "    return kink_RUL"
      ],
      "metadata": {
        "id": "kaLTLTuBV-Sj"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_rul_of_one_id(FD00X_of_one_id, max_cycle_rul=None):\n",
        "    '''\n",
        "    Enter the data of an engine_id of train_FD001 and output the corresponding RUL (remaining life) of these data.\n",
        "    type is list\n",
        "    '''\n",
        "\n",
        "    cycle_list = FD00X_of_one_id['cycle'].tolist()\n",
        "    if max_cycle_rul is None:\n",
        "        max_cycle = max(cycle_list)  # Failure cycle\n",
        "    else:\n",
        "        max_cycle = max(cycle_list) + max_cycle_rul\n",
        "        # print(max(cycle_list), max_cycle_rul)\n",
        "\n",
        "    # return kink_RUL(cycle_list,max_cycle)\n",
        "    return kink_RUL(cycle_list, max_cycle)\n"
      ],
      "metadata": {
        "id": "Y3zQpiJ9WB5G"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_rul_of_one_file(FD00X, id='engine_id', RUL_FD00X=None):\n",
        "    '''\n",
        "    Input train_FD001, output a list\n",
        "    '''\n",
        "    rul = []\n",
        "    # In the loop train, each id value of the 'engine_id' column\n",
        "    if RUL_FD00X is None:\n",
        "        for _id in set(FD00X[id]):\n",
        "            rul.extend(compute_rul_of_one_id(FD00X[FD00X[id] == _id]))\n",
        "        return rul\n",
        "    else:\n",
        "        rul = []\n",
        "        for _id in set(FD00X[id]):\n",
        "            # print(\"#### id ####\", int(RUL_FD00X.iloc[_id - 1]))\n",
        "            true_rul.append(int(RUL_FD00X.iloc[_id - 1]))\n",
        "            rul.extend(compute_rul_of_one_id(FD00X[FD00X[id] == _id], int(RUL_FD00X.iloc[_id - 1])))\n",
        "        return rul\n"
      ],
      "metadata": {
        "id": "D3V117cVaNZA"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path='/content/CMAPSS/'"
      ],
      "metadata": {
        "id": "GLG45nPJbrf-"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(save_training_data, save_testing_data):\n",
        "    if save == False:  # Use '==' for comparison instead of '=' for assignment\n",
        "        return (\n",
        "            np.load(\"normalized_train_data.npy\"),\n",
        "            np.load(\"normalized_test_data.npy\"),\n",
        "            pd.read_csv(\"normalized_train_data.csv\", index_col=[0]),  # Fixed the quotes around filenames\n",
        "            pd.read_csv(\"normalized_test_data.csv\", index_col=[0]),  # Fixed the quotes around filenames\n",
        "        )\n",
        "\n",
        "    column_names = [\n",
        "        'engine_id', 'cycle', 'setting1', 'setting2', 'setting3',  # Corrected the variable name\n",
        "        's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10',\n",
        "        's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21'\n",
        "    ]\n",
        "\n",
        "    if save_training_data:  ### Training ###\n",
        "        train_FD001 = pd.read_table('/content/CMAPSS/train_FD001.txt/', header=None, delim_whitespace=True)\n",
        "        train_FD002 = pd.read_table('/content/CMAPSS/train_FD002.txt/', header=None, delim_whitespace=True)\n",
        "        train_FD003 = pd.read_table('/content/CMAPSS/train_FD003.txt/', header=None, delim_whitespace=True)\n",
        "        train_FD004 = pd.read_table('/content/CMAPSS/train_FD004.txt/', header=None, delim_whitespace=True)\n",
        "        train_FD001.columns = column_names  # Corrected the variable name\n",
        "        train_FD002.columns = column_names  # Corrected the variable name\n",
        "        train_FD003.columns = column_names  # Corrected the variable name\n",
        "        train_FD004.columns = column_names  # Corrected the variable name\n",
        "        previous_len = 0\n",
        "        frames = []\n",
        "        for data_file in ['train_FD00' + str(i) for i in files]:  # load subdataset by subdataset\n",
        "\n",
        "            #### standard normalization ####\n",
        "            mean = eval(data_file).iloc[:, 2:len(list(eval(data_file)))].mean()\n",
        "            std = eval(data_file).iloc[:, 2:len(list(eval(data_file)))].std()\n",
        "            std.replace(0, 1, inplace=True)\n",
        "            # print(\"std\", std)\n",
        "            ################################\n",
        "            if min_max_norm:\n",
        "                scaler = MinMaxScaler()\n",
        "                eval(data_file).iloc[:, 2:len(list(eval(data_file)))] = scaler.fit_transform(\n",
        "                    eval(data_file).iloc[:, 2:len(list(eval(data_file)))])\n",
        "            else:\n",
        "                eval(data_file).iloc[:, 2:len(list(eval(data_file)))] = (eval(data_file).iloc[:, 2:len(\n",
        "                    list(eval(data_file)))] - mean) / std\n",
        "\n",
        "            eval(data_file)['RUL'] = compute_rul_of_one_file(eval(data_file))\n",
        "            current_len = len(eval(data_file))\n",
        "            # print(eval(data_file).index)\n",
        "            eval(data_file).index = range(previous_len, previous_len + current_len)\n",
        "            previous_len = previous_len + current_len\n",
        "            # print(eval(data_file).index)\n",
        "            frames.append(eval(data_file))\n",
        "            print(data_file)\n",
        "\n",
        "        train = pd.concat(frames)\n",
        "        global training_engine_id\n",
        "        training_engine_id = train['engine_id']\n",
        "        train = train.drop('engine_id', 1)\n",
        "        train = train.drop('cycle', 1)\n",
        "        # if files[0] == 1 or files[0] == 3:\n",
        "        #     train = train.drop('setting3', 1)\n",
        "        #     train = train.drop('s18', 1)\n",
        "        #     train = train.drop('s19', 1)\n",
        "\n",
        "        train_values = train.values * SCALE\n",
        "        np.save('normalized_train_data.npy', train_values)\n",
        "        train.to_csv('normalized_train_data.csv')\n",
        "        ###########\n",
        "\n",
        "    else:\n",
        "        train = pd.read_csv('normalized_train_data.csv', index_col=[0])\n",
        "        train_values = train.values\n",
        "\n",
        "    if save_testing_data:  ### testing ###\n",
        "        test_FD001 = pd.read_table('/content/CMAPSS/test_FD001.txt/', header=None, delim_whitespace=True)\n",
        "        test_FD002 = pd.read_table('/content/CMAPSS/test_FD002.txt/', header=None, delim_whitespace=True)\n",
        "        test_FD003 = pd.read_table('/content/CMAPSS/test_FD003.txt/', header=None, delim_whitespace=True)\n",
        "        test_FD004 = pd.read_table('/content/CMAPSS/test_FD004.txt/', header=None, delim_whitespace=True)\n",
        "        test_FD001.columns = column_names\n",
        "        test_FD002.columns = column_names\n",
        "        test_FD003.columns = column_names\n",
        "        test_FD004.columns = column_names\n",
        "        RUL_FD001 = pd.read_table('/content/CMAPSS/RUL_FD001.txt/', header=None, delim_whitespace=True)\n",
        "        RUL_FD002 = pd.read_table('/content/CMAPSS/RUL_FD002.txt/', header=None, delim_whitespace=True)\n",
        "        RUL_FD003 = pd.read_table('/content/CMAPSS/RUL_FD003.txt/', header=None, delim_whitespace=True)\n",
        "        RUL_FD004 = pd.read_table('/content/CMAPSS/RUL_FD004.txt/', header=None, delim_whitespace=True)\n",
        "        RUL_FD001.columns = ['RUL']\n",
        "        RUL_FD002.columns = ['RUL']\n",
        "        RUL_FD003.columns = ['RUL']\n",
        "        RUL_FD004.columns = ['RUL']\n",
        "        previous_len = 0\n",
        "        frames = []\n",
        "        for (data_file, rul_file) in [('test_FD00' + str(i), 'RUL_FD00' + str(i)) for i in files]:\n",
        "            mean = eval(data_file).iloc[:, 2:len(list(eval(data_file)))].mean()\n",
        "            std = eval(data_file).iloc[:, 2:len(list(eval(data_file)))].std()\n",
        "            std.replace(0, 1, inplace=True)\n",
        "\n",
        "            if min_max_norm:\n",
        "                scaler = MinMaxScaler()\n",
        "                eval(data_file).iloc[:, 2:len(list(eval(data_file)))] = scaler.fit_transform(\n",
        "                    eval(data_file).iloc[:, 2:len(list(eval(data_file)))])\n",
        "            else:\n",
        "                eval(data_file).iloc[:, 2:len(list(eval(data_file)))] = (eval(data_file).iloc[:, 2:len(\n",
        "                    list(eval(data_file)))] - mean) / std\n",
        "\n",
        "            eval(data_file)['RUL'] = compute_rul_of_one_file(eval(data_file), RUL_FD00X=eval(rul_file))\n",
        "            current_len = len(eval(data_file))\n",
        "            eval(data_file).index = range(previous_len, previous_len + current_len)\n",
        "            previous_len = previous_len + current_len\n",
        "            frames.append(eval(data_file))\n",
        "            print(data_file)\n",
        "            if len(files) == 1:\n",
        "                global test_engine_id\n",
        "                test_engine_id = eval(data_file)['engine_id']\n",
        "\n",
        "        test = pd.concat(frames)\n",
        "        test = test.drop('engine_id', 1)\n",
        "        test = test.drop('cycle', 1)\n",
        "        # if files[0] == 1 or files[0] == 3:\n",
        "        #     test = test.drop('setting3', 1)\n",
        "        #     test = test.drop('s18', 1)\n",
        "        #     test = test.drop('s19', 1)\n",
        "\n",
        "        test_values = test.values * SCALE\n",
        "        np.save('normalized_test_data.npy', test_values)\n",
        "        test.to_csv('normalized_test_data.csv')\n",
        "        ###########\n",
        "\n",
        "    else:\n",
        "        test = pd.read_csv('normalized_test_data.csv', index_col=[0])\n",
        "        test_values = test.values\n",
        "\n",
        "    return train_values, test_values, train, test\n"
      ],
      "metadata": {
        "id": "xO_gxVjKaUJN"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_names = ['unit_number', 'time_cycles']\n",
        "setting_names = ['setting_1', 'setting_2', 'setting_3']\n",
        "sensor_names = ['sensor_{}'.format(i) for i in range(1,22)]\n",
        "col_names = index_names + setting_names + sensor_names"
      ],
      "metadata": {
        "id": "OQ8iYISYajdx"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/PHM08.zip\" \"/content\"\n",
        "!unzip PHM08.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oba8iz9wWFC2",
        "outputId": "9584108b-576b-49c0-f1bc-682f018044eb"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  PHM08.zip\n",
            "  inflating: PHM08/final_test.txt    \n",
            "  inflating: PHM08/test.txt          \n",
            "  inflating: PHM08/train.txt         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_PHM08Data(save=False):\n",
        "    \"\"\"\n",
        "    Function is to load PHM 2008 challenge dataset\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if save == False:\n",
        "        return np.load(\"./PHM08/processed_data/phm_training_data.npy\"), np.load(\"./PHM08/processed_data/phm_testing_data.npy\"), np.load(\n",
        "            \"./PHM08/processed_data/phm_original_testing_data.npy\")\n",
        "\n",
        "    column_name = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "                   's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "                   's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "    phm_training_data = pd.read_table('/content/PHM08/train.txt/', header=None, delim_whitespace=True)\n",
        "    phm_training_data.columns = column_name\n",
        "    phm_testing_data = pd.read_table('/content/PHM08/test.txt/', header=None, delim_whitespace=True)\n",
        "    phm_testing_data.columns = column_name\n",
        "    print(\"phm training\")\n",
        "    mean = phm_training_data.iloc[:, 2:len(list(phm_training_data))].mean()\n",
        "    std = phm_training_data.iloc[:, 2:len(list(phm_training_data))].std()\n",
        "    phm_training_data.iloc[:, 2:len(list(phm_training_data))] = (phm_training_data.iloc[:, 2:len(\n",
        "        list(phm_training_data))] - mean) / std\n",
        "    phm_training_data['RUL'] = compute_rul_of_one_file(phm_training_data)\n",
        "\n",
        "    print(\"phm testing\")\n",
        "    mean = phm_testing_data.iloc[:, 2:len(list(phm_testing_data))].mean()\n",
        "    std = phm_testing_data.iloc[:, 2:len(list(phm_testing_data))].std()\n",
        "    phm_testing_data.iloc[:, 2:len(list(phm_testing_data))] = (phm_testing_data.iloc[:, 2:len(\n",
        "        list(phm_testing_data))] - mean) / std\n",
        "    phm_testing_data['RUL'] = 0\n",
        "\n",
        "    train_engine_id = phm_training_data['engine_id']\n",
        "    phm_training_data = phm_training_data.drop('engine_id', 1)\n",
        "    phm_training_data = phm_training_data.drop('cycle', 1)\n",
        "\n",
        "    global test_engine_id\n",
        "    test_engine_id = phm_testing_data['engine_id']\n",
        "    phm_testing_data = phm_testing_data.drop('engine_id', 1)\n",
        "    phm_testing_data = phm_testing_data.drop('cycle', 1)\n",
        "\n",
        "    phm_training_data = phm_training_data.values\n",
        "    phm_testing_data = phm_testing_data.values\n",
        "\n",
        "    engine_ids = train_engine_id.unique()\n",
        "    train_test_split = np.random.rand(len(engine_ids)) < 0.80\n",
        "    train_engine_ids = engine_ids[train_test_split]\n",
        "    test_engine_ids = engine_ids[~train_test_split]\n",
        "    training_data = phm_training_data[train_engine_id[train_engine_id == train_engine_ids[0]].index]\n",
        "    for id in train_engine_ids[1:]:\n",
        "        tmp = phm_training_data[train_engine_id[train_engine_id == id].index]\n",
        "        training_data = np.concatenate((training_data, tmp))\n",
        "\n",
        "    testing_data = phm_training_data[train_engine_id[train_engine_id == test_engine_ids[0]].index]\n",
        "    for id in test_engine_ids[1:]:\n",
        "        tmp = phm_training_data[train_engine_id[train_engine_id == id].index]\n",
        "        testing_data = np.concatenate((testing_data, tmp))\n",
        "\n",
        "    np.save(\"./PHM08/processed_data/phm_training_data.npy\", training_data)\n",
        "    np.savetxt(\"./PHM08/processed_data/phm_training_data.txt\", training_data, delimiter=\" \")\n",
        "    np.save(\"./PHM08/processed_data/phm_testing_data.npy\", testing_data)\n",
        "    np.savetxt(\"./PHM08/processed_data/phm_testing_data.txt\", testing_data, delimiter=\" \")\n",
        "    np.save(\"./PHM08/processed_data/phm_original_testing_data.npy\", phm_testing_data)\n",
        "    np.savetxt(\"./PHM08/processed_data/phm_original_testing_data.csv\", phm_testing_data, delimiter=\",\")\n",
        "\n",
        "    return training_data, testing_data, phm_testing_data\n"
      ],
      "metadata": {
        "id": "arupLgDIjObY"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_augmentation(files=1, low=[10, 40, 90, 170], high=[35, 85, 160, 250], plot=False, combine=False):\n",
        "    '''\n",
        "    This helper function only augments the training data to look like testing data.\n",
        "    Training data always run to a failure. But testing data is mostly stopped before a failure.\n",
        "    Therefore, training data augmented to have scenarios without failure\n",
        "\n",
        "    :param files: select which sub CMPASS dataset\n",
        "    :param low: lower bound for the random selection of the engine cycle\n",
        "    :param high: upper bound for the random selection of the engine cycle\n",
        "    :param plot: switch to plot the augmented data\n",
        "    :return:\n",
        "    '''\n",
        "\n",
        "    DEBUG = False\n",
        "    column_name = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "                   's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "                   's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "\n",
        "    ### Loading original data ###\n",
        "    if files == \"phm\":\n",
        "        train_FD00x = pd.read_table(\"./PHM08/processed_data/phm_training_data.txt\", header=None, delim_whitespace=True)\n",
        "        train_FD00x.drop(train_FD00x.columns[len(train_FD00x.columns) - 1], axis=1, inplace=True)\n",
        "        train_FD00x.columns = column_name\n",
        "    else:\n",
        "        if combine:\n",
        "            train_FD00x,_,_ = combine_FD001_and_FD003()\n",
        "        else:\n",
        "            file_path = \"./CMAPSSData/train_FD00\" + str(files) + \".txt\"\n",
        "            train_FD00x = pd.read_table(file_path, header=None, delim_whitespace=True)\n",
        "            train_FD00x.columns = column_name\n",
        "            print(file_path.split(\"/\")[-1])\n",
        "        ### Standard Normal ###\n",
        "        mean = train_FD00x.iloc[:, 2:len(list(train_FD00x))].mean()\n",
        "        std = train_FD00x.iloc[:, 2:len(list(train_FD00x))].std()\n",
        "        std.replace(0, 1, inplace=True)\n",
        "        train_FD00x.iloc[:, 2:len(list(train_FD00x))] = (train_FD00x.iloc[:, 2:len(list(train_FD00x))] - mean) / std\n",
        "\n",
        "    final_train_FD = train_FD00x.copy()\n",
        "    previous_len = 0\n",
        "    frames = []\n",
        "    for i in range(len(high)):\n",
        "        train_FD = train_FD00x.copy()\n",
        "        train_engine_id = train_FD['engine_id']\n",
        "        engine_ids = train_engine_id.unique()\n",
        "        total_ids = len(engine_ids)\n",
        "        train_rul = []\n",
        "        print(\"*************\", final_train_FD.shape, total_ids, low[i], high[i], \"*****************\")\n",
        "        for id in range(1, total_ids + 1):\n",
        "\n",
        "            train_engine_id = train_FD['engine_id']\n",
        "            indexes = train_engine_id[train_engine_id == id].index  ### filter indexes related to id\n",
        "            traj_data = train_FD.loc[indexes]  ### filter trajectory data\n",
        "\n",
        "            cutoff_cycle = random.randint(low[i], high[i])  ### randomly selecting the cutoff point of the engine cycle\n",
        "\n",
        "            if cutoff_cycle > max(traj_data['cycle']):\n",
        "                cutoff_cycle = max(traj_data['cycle'])\n",
        "\n",
        "            train_rul.append(max(traj_data['cycle']) - cutoff_cycle)  ### collecting remaining cycles\n",
        "\n",
        "            cutoff_cycle_index = traj_data['cycle'][traj_data['cycle'] == cutoff_cycle].index  ### cutoff cycle index\n",
        "\n",
        "            if DEBUG:\n",
        "                print(\"traj_shape: \", traj_data.shape, \"current_engine_id:\", id, \"cutoff_cycle:\", cutoff_cycle,\n",
        "                      \"cutoff_index\", cutoff_cycle_index, \"engine_fist_index\", indexes[0], \"engine_last_index\",\n",
        "                      indexes[-1])\n",
        "            ### removing rows after cutoff cycle index ###\n",
        "            if cutoff_cycle_index[0] != indexes[-1]:\n",
        "                drop_range = list(range(cutoff_cycle_index[0] + 1, indexes[-1] + 1))\n",
        "                train_FD.drop(train_FD.index[drop_range], inplace=True)\n",
        "                train_FD.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        ### calculating the RUL for augmented data\n",
        "        train_rul = pd.DataFrame.from_dict({'RUL': train_rul})\n",
        "        train_FD['RUL'] = compute_rul_of_one_file(train_FD, RUL_FD00X=train_rul)\n",
        "\n",
        "        ### changing the engine_id for augmented data\n",
        "        train_engine_id = train_FD['engine_id']\n",
        "        for id in range(1, total_ids + 1):\n",
        "            indexes = train_engine_id[train_engine_id == id].index\n",
        "            train_FD.loc[indexes, 'engine_id'] = id + total_ids * (i + 1)\n",
        "\n",
        "        if i == 0:  # should only execute at the first iteration\n",
        "            final_train_FD['RUL'] = compute_rul_of_one_file(final_train_FD)\n",
        "            current_len = len(final_train_FD)\n",
        "            final_train_FD.index = range(previous_len, previous_len + current_len)\n",
        "            previous_len = previous_len + current_len\n",
        "        ### Re-indexing the augmented data\n",
        "        train_FD['RUL'].index = range(previous_len, previous_len + len(train_FD))\n",
        "        previous_len = previous_len + len(train_FD)\n",
        "\n",
        "        final_train_FD = pd.concat(\n",
        "            [final_train_FD, train_FD])  # concatenate the newly augmented data with previous data\n",
        "\n",
        "    frames.append(final_train_FD)\n",
        "    train = pd.concat(frames)\n",
        "    train.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    train_engine_id = train['engine_id']\n",
        "    # print(train_engine_id)\n",
        "    engine_ids = train_engine_id.unique()\n",
        "    # print(engine_ids[1:])\n",
        "    np.random.shuffle(engine_ids)\n",
        "    # print(engine_ids)\n",
        "\n",
        "    training_data = train.loc[train_engine_id[train_engine_id == engine_ids[0]].index]\n",
        "    training_data.reset_index(drop=True, inplace=True)\n",
        "    previous_len = len(training_data)\n",
        "    for id in engine_ids[1:]:\n",
        "        traj_data = train.loc[train_engine_id[train_engine_id == id].index]\n",
        "        current_len = len(traj_data)\n",
        "        traj_data.index = range(previous_len, previous_len + current_len)\n",
        "        previous_len = previous_len + current_len\n",
        "        training_data = pd.concat([training_data, traj_data])\n",
        "\n",
        "\n",
        "    global training_engine_id\n",
        "    training_engine_id = training_data['engine_id']\n",
        "\n",
        "    training_data = training_data.drop('engine_id', 1)\n",
        "    training_data = training_data.drop('cycle', 1)\n",
        "    # if files == 1 or files == 3:\n",
        "    #     training_data = training_data.drop('setting3', 1)\n",
        "    #     training_data = training_data.drop('s18', 1)\n",
        "    #     training_data = training_data.drop('s19', 1)\n",
        "\n",
        "    training_data_values = training_data.values * SCALE\n",
        "    np.save('normalized_train_data.npy', training_data_values)\n",
        "    training_data.to_csv('normalized_train_data.csv')\n",
        "\n",
        "\n",
        "    train = training_data_values\n",
        "    x_train = train[:, :train.shape[1] - 1]\n",
        "    y_train = train[:, train.shape[1] - 1] * RESCALE\n",
        "    print(\"training in augmentation\", x_train.shape, y_train.shape)\n",
        "\n",
        "    if plot:\n",
        "        plt.plot(y_train, label=\"train\")\n",
        "\n",
        "        plt.figure()\n",
        "        plt.plot(x_train)\n",
        "        plt.title(\"train\")\n",
        "        # plt.figure()\n",
        "          # plt.plot(y_train)\n",
        "        # plt.title(\"test\")\n",
        "\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "m4yWmadeXq0k"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyse_Data(dataset, files=None, plot=True, min_max=False):\n",
        "    '''\n",
        "    Generate pre-processed data according to the given dataset\n",
        "    :param dataset: choose between \"phm\" for PHM 2008 dataset or \"cmapss\" for CMAPSS data set with file number\n",
        "    :param files: Only for CMAPSS dataset to select sub dataset\n",
        "    :param min_max: switch to allow min-max normalization\n",
        "    :return:\n",
        "    '''\n",
        "\n",
        "    if dataset == \"phm\":\n",
        "        training_data, testing_data, phm_testing_data = get_PHM08Data(save=True)\n",
        "\n",
        "        x_phmtrain = training_data[:, :training_data.shape[1] - 1]\n",
        "        y_phmtrain = training_data[:, training_data.shape[1] - 1]\n",
        "\n",
        "        x_phmtest = testing_data[:, :testing_data.shape[1] - 1]\n",
        "        y_phmtest = testing_data[:, testing_data.shape[1] - 1]\n",
        "        print(\"phmtrain\", x_phmtrain.shape, y_phmtrain.shape)\n",
        "\n",
        "        print(\"phmtest\", x_phmtrain.shape, y_phmtrain.shape)\n",
        "        print(\"phmtest\", phm_testing_data.shape)\n",
        "\n",
        "        if plot:\n",
        "            # plt.plot(x_phmtrain, label=\"phmtrain_x\")\n",
        "            plt.figure()\n",
        "            plt.plot(y_phmtrain, label=\"phmtrain_y\")\n",
        "\n",
        "            # plt.figure()\n",
        "            # plt.plot(x_phmtest, label=\"phmtest_x\")\n",
        "            plt.figure()\n",
        "            plt.plot(y_phmtest, label=\"phmtest_y\")\n",
        "\n",
        "            # plt.figure()\n",
        "            # plt.plot(phm_testing_data, label=\"test\")\n",
        "            plt.show()\n",
        "    elif dataset == \"cmapss\":\n",
        "        training_data, testing_data, training_pd, testing_pd = get_CMAPSSData(save=True, files=files,\n",
        "                                                                              min_max_norm=min_max)\n",
        "        x_train = training_data[:, :training_data.shape[1] - 1]\n",
        "        y_train = training_data[:, training_data.shape[1] - 1]\n",
        "        print(\"training\", x_train.shape, y_train.shape)\n",
        "\n",
        "        x_test = testing_data[:, :testing_data.shape[1] - 1]\n",
        "        y_test = testing_data[:, testing_data.shape[1] - 1]\n",
        "        print(\"testing\", x_test.shape, y_test.shape)\n",
        "        if plot:\n",
        "            plt.plot(y_train, label=\"train\")\n",
        "            plt.figure()\n",
        "            plt.plot(y_test, label=\"test\")\n",
        "\n",
        "            plt.figure()\n",
        "            plt.plot(x_train)\n",
        "            plt.title(\"train: FD00\" + str(files[0]))\n",
        "            plt.figure()\n",
        "            plt.plot(y_train)\n",
        "            plt.title(\"train: FD00\" + str(files[0]))\n",
        "            plt.show()\n"
      ],
      "metadata": {
        "id": "gT1xveDXmJhX"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_FD001_and_FD003():\n",
        "    column_name = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "                   's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "                   's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "\n",
        "    train_FD001 = pd.read_table('/content/CMAPSS/train_FD001.txt/', header=None, delim_whitespace=True)\n",
        "    train_FD003 = pd.read_table('/content/CMAPSS/train_FD003.txt/', header=None, delim_whitespace=True)\n",
        "    train_FD001.columns = column_name\n",
        "    train_FD003.columns = column_name\n",
        "\n",
        "    FD001_max_engine_id = max(train_FD001['engine_id'])\n",
        "    train_FD003['engine_id'] = train_FD003['engine_id'] + FD001_max_engine_id\n",
        "    train_FD003.index = range(len(train_FD001), len(train_FD001) + len(train_FD003))\n",
        "    train_FD001_FD002 = pd.concat([train_FD001,train_FD003])\n",
        "\n",
        "    test_FD001 = pd.read_table('/content/CMAPSS/test_FD001.txt/', header=None, delim_whitespace=True)\n",
        "    test_FD003 = pd.read_table('/content/CMAPSS/test_FD003.txt/', header=None, delim_whitespace=True)\n",
        "    test_FD001.columns = column_name\n",
        "    test_FD003.columns = column_name\n",
        "    FD001_max_engine_id = max(test_FD001['engine_id'])\n",
        "    test_FD003['engine_id'] = test_FD003['engine_id'] + FD001_max_engine_id\n",
        "    test_FD003.index = range(len(test_FD001), len(test_FD001) + len(test_FD003))\n",
        "    test_FD001_FD002 = pd.concat([test_FD001,test_FD003])\n",
        "\n",
        "    RUL_FD001 = pd.read_table('/content/CMAPSS/RUL_FD001.txt/', header=None, delim_whitespace=True)\n",
        "    RUL_FD003 = pd.read_table('/content/CMAPSS/RUL_FD003.txt/', header=None, delim_whitespace=True)\n",
        "    RUL_FD001.columns = ['RUL']\n",
        "    RUL_FD003.columns = ['RUL']\n",
        "    RUL_FD003.index = range(len(RUL_FD001), len(RUL_FD001) + len(RUL_FD003))\n",
        "    RUL_FD001_FD002 = pd.concat([test_FD001, test_FD003])\n",
        "\n",
        "    return train_FD001_FD002,test_FD001_FD002,RUL_FD001_FD002"
      ],
      "metadata": {
        "id": "q3yB_senmwhs"
      },
      "execution_count": 65,
      "outputs": []
    }
  ]
}