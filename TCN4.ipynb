{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwmQZ/ScWx8Y6CjxIzz5AZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swastik200/30DaysOfDSA/blob/main/TCN4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x69cuUpuwiwH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU\n",
        "from tensorflow.keras.callbacks import TensorBoard, LearningRateScheduler\n",
        "import time"
      ],
      "metadata": {
        "id": "YjoZ84ZZxSZR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zskTzvMsxUdS",
        "outputId": "c23f7c4b-c94c-4346-cdce-4c19e13aff39"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/CMAPSS.zip\" \"/content\"\n",
        "!unzip CMAPSS.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4sHYPXlxUfF",
        "outputId": "69cd5f6c-44a5-4c37-9a76-816cc889a5bc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  CMAPSS.zip\n",
            "  inflating: CMAPSS/readme.txt       \n",
            "  inflating: CMAPSS/RUL_FD001.txt    \n",
            "  inflating: CMAPSS/RUL_FD002.txt    \n",
            "  inflating: CMAPSS/RUL_FD003.txt    \n",
            "  inflating: CMAPSS/RUL_FD004.txt    \n",
            "  inflating: CMAPSS/test_FD001.txt   \n",
            "  inflating: CMAPSS/test_FD002.txt   \n",
            "  inflating: CMAPSS/test_FD003.txt   \n",
            "  inflating: CMAPSS/test_FD004.txt   \n",
            "  inflating: CMAPSS/train_FD001.txt  \n",
            "  inflating: CMAPSS/train_FD002.txt  \n",
            "  inflating: CMAPSS/train_FD003.txt  \n",
            "  inflating: CMAPSS/train_FD004.txt  \n",
            "  inflating: CMAPSS/x.txt            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/CMAPSS/'\n",
        "col_names = ['unit_number', 'time_cycles', 'setting_1', 'setting_2', 'setting_3'] + ['sensor_{}'.format(i) for i in range(1, 22)]\n",
        "df_train = pd.read_csv(path+'train_FD004.txt', sep='\\s+', header=None, names=col_names)\n",
        "df_test = pd.read_csv(path+'test_FD004.txt', sep='\\s+', header=None, names=col_names)\n",
        "y_test = pd.read_csv(path+'RUL_FD004.txt', sep='\\s+', header=None, names=['RUL'])\n"
      ],
      "metadata": {
        "id": "cS6uNUmRxefA"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_targets(data_length, early_rul):\n",
        "    early_rul_duration = data_length - early_rul\n",
        "    if early_rul_duration <= 0:\n",
        "        return np.arange(data_length - 1, -1, -1)\n",
        "    else:\n",
        "        new_early_rul = early_rul * np.ones(early_rul_duration)\n",
        "        origin_rul = np.arange(early_rul - 1, -1, -1)\n",
        "        return np.append(new_early_rul, origin_rul)"
      ],
      "metadata": {
        "id": "5hAhSfdGxmeb"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def process_input_data_with_targets(input_data, target_data, window_length, shift):\n",
        "    num_batches = int(np.floor((len(input_data) - window_length) / shift)) + 1\n",
        "    num_features = input_data.shape[1]\n",
        "    output = np.repeat(np.nan, repeats=num_batches * window_length * num_features)\n",
        "    output_data = output.reshape(num_batches, window_length, num_features)\n",
        "\n",
        "    if target_data is None:\n",
        "        for batch in range(num_batches):\n",
        "            output_data[batch, :, :] = input_data[(0 + shift * batch):(0 + shift * batch + window_length), :]\n",
        "        return output_data\n",
        "    else:\n",
        "        output_targets = np.repeat(np.nan, repeats=num_batches)\n",
        "        for batch in range(num_batches):\n",
        "            window_start = shift * batch\n",
        "            window_end = window_start + window_length\n",
        "\n",
        "            output_data[batch, :, :] = input_data[window_start:window_end, :]\n",
        "            output_targets[batch] = target_data[window_end - 1]\n",
        "        return output_data, output_targets"
      ],
      "metadata": {
        "id": "Dm6tBynRxpKE"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_test_data(test_data_for_an_engine, window_length, shift, num_test_windows=1):\n",
        "    max_num_test_batches = int(np.floor((len(test_data_for_an_engine) - window_length) / shift)) + 1\n",
        "\n",
        "    if max_num_test_batches < num_test_windows:\n",
        "        required_len = (max_num_test_batches - 1) * shift + window_length\n",
        "        batched_test_data_for_an_engine = process_input_data_with_targets(\n",
        "            test_data_for_an_engine[-required_len:, :],\n",
        "            target_data=None,\n",
        "            window_length=window_length,\n",
        "            shift=shift\n",
        "        )\n",
        "        return batched_test_data_for_an_engine, max_num_test_batches\n",
        "    else:\n",
        "        required_len = (num_test_windows - 1) * shift + window_length\n",
        "        batched_test_data_for_an_engine = process_input_data_with_targets(\n",
        "            test_data_for_an_engine[-required_len:, :],\n",
        "            target_data=None,\n",
        "            window_length=window_length,\n",
        "            shift=shift\n",
        "        )\n",
        "        return batched_test_data_for_an_engine, num_test_windows"
      ],
      "metadata": {
        "id": "tZyW6YJCxwTV"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_first_column = df_train[\"unit_number\"]\n",
        "test_data_first_column = df_test[\"unit_number\"]"
      ],
      "metadata": {
        "id": "ub_K9kG-xzUq"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "\n",
        "train_data = scaler.fit_transform(df_train.drop(columns=['unit_number', 'setting_1', 'setting_2', 'sensor_1', 'sensor_5', 'sensor_6', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']))\n",
        "test_data = scaler.transform(df_test.drop(columns=['unit_number', 'setting_1', 'setting_2', 'sensor_1', 'sensor_5', 'sensor_6', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']))"
      ],
      "metadata": {
        "id": "N6zZmrBixzXl"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.DataFrame(data=np.c_[train_data_first_column, train_data])\n",
        "test_data = pd.DataFrame(data=np.c_[test_data_first_column, test_data])"
      ],
      "metadata": {
        "id": "aOGVBY9Ox54V"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_machines = len(train_data[0].unique())\n",
        "num_test_machines = len(test_data[0].unique())\n",
        "window_length = 30\n",
        "shift = 1\n",
        "early_rul = 125\n",
        "num_test_windows = 5\n"
      ],
      "metadata": {
        "id": "lUsQzOA_xiZL"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_train_data = []\n",
        "processed_train_targets = []\n"
      ],
      "metadata": {
        "id": "iWBtPTHRxibV"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in np.arange(1, num_train_machines + 1):\n",
        "    temp_train_data = train_data[train_data[0] == i].drop(columns=[0]).values\n",
        "    temp_train_targets = process_targets(data_length=temp_train_data.shape[0], early_rul=early_rul)\n",
        "    data_for_a_machine, targets_for_a_machine = process_input_data_with_targets(\n",
        "        temp_train_data, temp_train_targets, window_length=window_length, shift=shift\n",
        "    )\n",
        "    processed_train_data.append(data_for_a_machine)\n",
        "    processed_train_targets.append(targets_for_a_machine)\n",
        "\n",
        "processed_train_data = np.concatenate(processed_train_data)\n",
        "processed_train_targets = np.concatenate(processed_train_targets)\n"
      ],
      "metadata": {
        "id": "NOxjCZnhxidn"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_test_data = []\n",
        "num_test_windows_list = []\n",
        "\n",
        "for i in range(1, num_test_machines + 1):  # Use range instead of np.arange\n",
        "    temp_test_data = test_data[test_data[0] == i].drop(columns=[0]).values\n",
        "\n",
        "    # Prepare test data\n",
        "    if temp_test_data.shape[0] >= window_length:  # Ensure temp_test_data has enough rows\n",
        "        test_data_for_an_engine, num_windows = process_test_data(temp_test_data, window_length=window_length, shift=shift, num_test_windows=num_test_windows)\n",
        "        processed_test_data.append(test_data_for_an_engine)\n",
        "        num_test_windows_list.append(num_windows)\n",
        "    else:\n",
        "        print(f\"Skipping engine {i} due to insufficient data for windowing.\")\n",
        "\n",
        "if processed_test_data:  # Check if processed_test_data is not empty\n",
        "    processed_test_data = np.concatenate(processed_test_data)\n",
        "    true_rul = y_test.values\n",
        "\n",
        "    # Shuffle training data\n",
        "    index = np.random.permutation(len(processed_train_targets))\n",
        "    processed_train_data, processed_train_targets = processed_train_data[index], processed_train_targets[index]\n",
        "else:\n",
        "    print(\"No valid data available after processing.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDZsTC9FyJTv",
        "outputId": "274042aa-1999-4b7b-f8ad-6eb4f4c3b14a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping engine 10 due to insufficient data for windowing.\n",
            "Skipping engine 19 due to insufficient data for windowing.\n",
            "Skipping engine 28 due to insufficient data for windowing.\n",
            "Skipping engine 125 due to insufficient data for windowing.\n",
            "Skipping engine 141 due to insufficient data for windowing.\n",
            "Skipping engine 156 due to insufficient data for windowing.\n",
            "Skipping engine 164 due to insufficient data for windowing.\n",
            "Skipping engine 204 due to insufficient data for windowing.\n",
            "Skipping engine 229 due to insufficient data for windowing.\n",
            "Skipping engine 239 due to insufficient data for windowing.\n",
            "Skipping engine 246 due to insufficient data for windowing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = np.random.permutation(len(processed_train_targets))\n",
        "processed_train_data, processed_train_targets = processed_train_data[index], processed_train_targets[index]"
      ],
      "metadata": {
        "id": "qzDC0TiKyVSQ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_train_data, processed_val_data, processed_train_targets, processed_val_targets = train_test_split(\n",
        "    processed_train_data, processed_train_targets, test_size=0.2, random_state=666\n",
        ")"
      ],
      "metadata": {
        "id": "0YIO5fOMyYC0"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GRU_model():\n",
        "    input_shape = (window_length, 15)\n",
        "    model = Sequential([\n",
        "        GRU(128, input_shape=input_shape, return_sequences=True, activation=\"tanh\"),\n",
        "        GRU(64, activation=\"tanh\", return_sequences=True),\n",
        "        GRU(32, activation=\"tanh\"),\n",
        "        Dense(96, activation=\"relu\"),\n",
        "        Dense(128, activation=\"relu\"),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "Q3Z_qG-GyYJh"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scheduler(epoch):\n",
        "    if epoch < 10:\n",
        "        return 0.001\n",
        "    else:\n",
        "        return 0.0001\n",
        "\n",
        "tf_callback = TensorBoard(log_dir=\"./logs\")\n",
        "callback = LearningRateScheduler(scheduler, verbose=0)\n",
        "\n",
        "batch_size = 50\n",
        "epochs = 20"
      ],
      "metadata": {
        "id": "3mHY91cXyYN6"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.layers import Reshape, Permute, concatenate\n",
        "\n",
        "def TCN_model():\n",
        "    input_shape = (window_length, 15)\n",
        "    kernel_size = 3\n",
        "    num_filters = 64\n",
        "    dilations = [1, 2, 4, 8, 16]\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = inputs  # Fix the indentation\n",
        "    skip_connections = []\n",
        "    for dilation_rate in dilations:\n",
        "        x = Conv1D(num_filters, kernel_size, padding='causal', dilation_rate=dilation_rate, activation='relu')(x)\n",
        "        skip_connections.append(x)\n",
        "\n",
        "    x = concatenate(skip_connections)\n",
        "    x = Conv1D(num_filters, kernel_size=1, padding='same', activation='relu')(x)\n",
        "    x = Conv1D(num_filters, kernel_size=1, padding='same', activation='relu')(x)\n",
        "    x = Conv1D(1, kernel_size=1, padding='same')(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "9Yo9BEUFyhCZ"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = { \"TCN\": TCN_model()}"
      ],
      "metadata": {
        "id": "TVaMvcUAylrb"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(true_rul ,pred_rul, model):\n",
        "  MAE = mean_absolute_error(true_rul, pred_rul)\n",
        "  RMSE = np.sqrt(mean_squared_error(true_rul, pred_rul))\n",
        "  r2 = r2_score(true_rul, pred_rul)\n",
        "  print(\"Testing : R-square = \",r2,'MAE = ',MAE,\"RMSE = \", RMSE)"
      ],
      "metadata": {
        "id": "q28wbrGvympe"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_train_loss = []\n",
        "history_val_loss = []\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(\"================\", model_name, \"================\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Slice the input data to match the expected shape\n",
        "    processed_train_data_sliced = processed_train_data[:, :, :15]\n",
        "    processed_val_data_sliced = processed_val_data[:, :, :15]\n",
        "\n",
        "    # Compile the model with run_eagerly=True\n",
        "    model.compile(optimizer='adam', loss='mse', run_eagerly=True)\n",
        "\n",
        "    model_history = model.fit(processed_train_data_sliced, processed_train_targets, epochs=epochs,\n",
        "                              validation_data=(processed_val_data_sliced, processed_val_targets),\n",
        "                              callbacks=[tf_callback, callback],\n",
        "                              batch_size=batch_size, verbose=0)\n",
        "    rul_pred = model.predict(processed_test_data[:, :, :15], verbose=0).reshape(-1)\n",
        "    end_time = time.time()\n",
        "\n",
        "    preds_for_each_engine = np.split(rul_pred, np.cumsum(num_test_windows_list)[:-1])\n",
        "    mean_pred_for_each_engine = [np.average(ruls_for_each_engine, weights=np.repeat(1 / num_windows, num_windows))\n",
        "                                  for ruls_for_each_engine, num_windows in zip(preds_for_each_engine,\n",
        "                                                                                  num_test_windows_list)]\n",
        "\n",
        "    print('Training : loss = ', model_history.history['loss'][-1])\n",
        "    print('Validation : loss = ', model_history.history['val_loss'][-1])\n",
        "\n",
        "    # Ensure true_rul and mean_pred_for_each_engine have the same length\n",
        "    true_rul_trimmed = true_rul[:len(mean_pred_for_each_engine)]\n",
        "    evaluate(true_rul_trimmed, mean_pred_for_each_engine, model_name)\n",
        "\n",
        "    history_train_loss.append(model_history.history['loss'])\n",
        "    history_val_loss.append(model_history.history['val_loss'])\n",
        "    print('Run Time :', int(end_time - start_time), 'sec')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyt11dQlyvBZ",
        "outputId": "6abdcb10-bad2-4cbf-a750-e5f621a4b51f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================ TCN ================\n",
            "Training : loss =  260.8281555175781\n",
            "Validation : loss =  270.0426940917969\n",
            "Testing : R-square =  -0.505995371976415 MAE =  53.75166777007691 RMSE =  66.51086435918846\n",
            "Run Time : 2123 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QdFqKvxtyJVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3BXhTKFOyJY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qWIpUWM5xigf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "76pr7eZDxUiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wp-zXfn3xUlt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}