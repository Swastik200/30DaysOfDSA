{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swastik200/30DaysOfDSA/blob/main/GA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8ZXc8Tt35V2",
        "outputId": "5386f37a-0c19-449d-e06c-fe39e8be2238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting deap\n",
            "  Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: deap\n",
            "Successfully installed deap-1.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow scikit-learn deap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c2O-o3wT3-iR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Convolution1D, MaxPooling1D, Dense, Dropout, LSTM, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from deap import base, creator, tools, algorithms\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FONHqDC4dCV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JK1nHsr94EQz",
        "outputId": "9f42775b-d2ab-416e-863a-d27c478ebc17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Archive:  CMAPSSData.zip\n",
            "  inflating: CMAPSS/RUL_FD001.txt    \n",
            "  inflating: CMAPSS/RUL_FD002.txt    \n",
            "  inflating: CMAPSS/RUL_FD003.txt    \n",
            "  inflating: CMAPSS/RUL_FD004.txt    \n",
            "  inflating: CMAPSS/test_FD001.txt   \n",
            "  inflating: CMAPSS/test_FD002.txt   \n",
            "  inflating: CMAPSS/test_FD003.txt   \n",
            "  inflating: CMAPSS/test_FD004.txt   \n",
            "  inflating: CMAPSS/train_FD001.txt  \n",
            "  inflating: CMAPSS/train_FD002.txt  \n",
            "  inflating: CMAPSS/train_FD003.txt  \n",
            "  inflating: CMAPSS/train_FD004.txt  \n",
            "  inflating: CMAPSS/x.txt            \n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp \"/content/drive/MyDrive/CMAPSSData.zip\" \"/content\"\n",
        "!unzip CMAPSSData.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1MzKYodV4e9-"
      },
      "outputs": [],
      "source": [
        "path='/content/CMAPSS/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tK7e17uH4fts"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "X-Y8V2UQ4fwE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ujNKIeI44f0N"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_data(path):\n",
        "    train_df = pd.read_csv(path+'train_FD001.txt', delim_whitespace=True, header=None)\n",
        "    test_df = pd.read_csv(path+'test_FD001.txt', delim_whitespace=True, header=None)\n",
        "    rul_df = pd.read_csv(path+'RUL_FD001.txt', delim_whitespace=True, header=None)\n",
        "\n",
        "    # Define column names\n",
        "    col_names = ['unit_number', 'time_in_cycles'] + [f'sensor_{i}' for i in range(1, 22)] + ['operational_setting_1', 'operational_setting_2', 'operational_setting_3']\n",
        "    train_df.columns = col_names\n",
        "    test_df.columns = col_names[:-3]  # Test data does not include the 'RUL' column\n",
        "    rul_df.columns = ['RUL']\n",
        "\n",
        "    return train_df, test_df, rul_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "me8mj3ZG9ydI"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(train_df, test_df):\n",
        "    # Normalize data\n",
        "    scaler = StandardScaler()\n",
        "    train_df.iloc[:, 2:] = scaler.fit_transform(train_df.iloc[:, 2:])\n",
        "    test_df.iloc[:, 2:] = scaler.transform(test_df.iloc[:, 2:])\n",
        "\n",
        "    # Generate labels for training data\n",
        "    rul = pd.DataFrame(train_df.groupby('unit_number')['time_in_cycles'].max()).reset_index()\n",
        "    rul.columns = ['unit_number', 'max']\n",
        "    train_df = train_df.merge(rul, on=['unit_number'], how='left')\n",
        "    train_df['RUL'] = train_df['max'] - train_df['time_in_cycles']\n",
        "    train_df.drop(columns=['max'], inplace=True)\n",
        "   # Generate sequences for LSTM\n",
        "    sequence_length = 50\n",
        "    def gen_sequence(id_df, seq_length):\n",
        "        data_matrix = id_df.values\n",
        "        num_elements = data_matrix.shape[0]\n",
        "        for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
        "            yield data_matrix[start:stop, :]\n",
        "\n",
        "    train_gen = (list(gen_sequence(train_df[train_df['unit_number'] == id], sequence_length))\n",
        "                for id in train_df['unit_number'].unique())\n",
        "    train_seq_array = np.concatenate(list(train_gen)).astype(np.float32)\n",
        "\n",
        "    train_label_gen = [train_df[train_df['unit_number'] == id]['RUL'].values[sequence_length:]\n",
        "                      for id in train_df['unit_number'].unique()]\n",
        "    train_label_array = np.concatenate(train_label_gen).astype(np.float32)\n",
        "\n",
        "    return train_seq_array, train_label_array, test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lugU4YWY-Ezb"
      },
      "outputs": [],
      "source": [
        "def createCNNLSTMModel(inputShape):\n",
        "    model = Sequential()\n",
        "    model.add(Convolution1D(input_shape=inputShape, filters=18, kernel_size=2, strides=1, padding='same', activation='relu', name='cv1'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='same', name='mp1'))\n",
        "\n",
        "    model.add(Convolution1D(filters=36, kernel_size=2, strides=1, padding='same', activation='relu', name='cv2'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='same', name='mp2'))\n",
        "\n",
        "    model.add(Convolution1D(filters=72, kernel_size=2, strides=1, padding='same', activation='relu', name='cv3'))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='same', name='mp3'))\n",
        "    model.add(tf.keras.layers.Bidirectional(LSTM(64, return_sequences=True)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Bidirectional(LSTM(64)))\n",
        "\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    model.compile(optimizer='rmsprop', loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GkzlgCTk-VCr"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    return rmse, r2, mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5On2QR2p_fxU"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_data(path):\n",
        "    train_df = pd.read_csv(path + 'train_FD001.txt', delim_whitespace=True, header=None)\n",
        "    test_df = pd.read_csv(path + 'test_FD001.txt', delim_whitespace=True, header=None)\n",
        "    rul_df = pd.read_csv(path + 'RUL_FD001.txt', delim_whitespace=True, header=None)\n",
        "\n",
        "    # Define column names\n",
        "    col_names = ['unit_number', 'time_in_cycles'] + [f'op_setting_{i}' for i in range(1, 4)] + [f'sensor_{i}' for i in range(1, 22)]\n",
        "\n",
        "    # Adding the columns to the dataframes\n",
        "    train_df.columns = col_names\n",
        "    test_df.columns = col_names\n",
        "\n",
        "    # Note: rul_df has only one column for RUL values\n",
        "    rul_df.columns = ['RUL']\n",
        "\n",
        "    return train_df, test_df, rul_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GH97W-ye-mtf"
      },
      "outputs": [],
      "source": [
        "path='/content/CMAPSS/'\n",
        "\n",
        "# Load and preprocess data\n",
        "train_df, test_df, rul_df = load_and_preprocess_data(path)\n",
        "train_seq_array, train_label_array, test_df = preprocess_data(train_df, test_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sySB_pHm-p10"
      },
      "outputs": [],
      "source": [
        "split = int(0.8 * len(train_seq_array))\n",
        "X_train, X_val = train_seq_array[:split], train_seq_array[split:]\n",
        "y_train, y_val = train_label_array[:split], train_label_array[split:]\n",
        "\n",
        "# Define input shape\n",
        "input_shape = X_train.shape[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5v7G_cRN_vEU"
      },
      "outputs": [],
      "source": [
        "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"attr_int\", random.randint, 1, 100)\n",
        "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_int, n=3)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "yQ1Ah3lRAG-i",
        "outputId": "f5fb58ac-5ea7-4254-fbf3-f519f1c71556"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "createCNNLSTMModel() missing 9 required positional arguments: 'f1', 'f2', 'f3', 'k', 'a1', 'a2', 'd1', 'd2', and 'lr'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-471877b95585>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"evaluate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mpopulation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meaSimple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcxpb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutpb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Extract best individual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deap/algorithms.py\u001b[0m in \u001b[0;36meaSimple\u001b[0;34m(population, toolbox, cxpb, mutpb, ngen, stats, halloffame, verbose)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0minvalid_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpopulation\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mfitnesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvalid_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitnesses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-471877b95585>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(individual)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindividual\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateCNNLSTMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs,\n\u001b[1;32m      8\u001b[0m               callbacks=[tf.keras.callbacks.EarlyStopping(patience=patience)], verbose=0)\n",
            "\u001b[0;31mTypeError\u001b[0m: createCNNLSTMModel() missing 9 required positional arguments: 'f1', 'f2', 'f3', 'k', 'a1', 'a2', 'd1', 'd2', and 'lr'"
          ]
        }
      ],
      "source": [
        "def evaluate(individual):\n",
        "    batch_size = individual[0]\n",
        "    epochs = individual[1]\n",
        "    patience = individual[2]\n",
        "\n",
        "    model = createCNNLSTMModel(input_shape)\n",
        "    model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs,\n",
        "              callbacks=[tf.keras.callbacks.EarlyStopping(patience=patience)], verbose=0)\n",
        "\n",
        "    rmse, r2, mae = evaluate_model(model, X_val, y_val)\n",
        "    return rmse,\n",
        "\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "toolbox.register(\"evaluate\", evaluate)\n",
        "population = toolbox.population(n=5)\n",
        "algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.2, ngen=5, verbose=True)\n",
        "\n",
        "# Extract best individual\n",
        "best_individual = tools.selBest(population, k=1)[0]\n",
        "batch_size, epochs, patience = best_individual\n",
        "print(f'Best hyperparameters: batch_size={batch_size}, epochs={epochs}, patience={patience}')\n",
        "\n",
        "# Train final model with best hyperparameters\n",
        "model = createCNNLSTMModel(input_shape)\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs,\n",
        "          callbacks=[tf.keras.callbacks.EarlyStopping(patience=patience)], verbose=1)\n",
        "\n",
        "# Evaluate on test data\n",
        "sequence_length = 50\n",
        "def gen_sequence(id_df, seq_length):\n",
        "    data_matrix = id_df.values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
        "        yield data_matrix[start:stop, :]\n",
        "\n",
        "test_gen = (list(gen_sequence(test_df[test_df['unit_number'] == id], sequence_length)) for id in test_df['unit_number'].unique())\n",
        "test_seq_array = np.concatenate(list(test_gen)).astype(np.float32)\n",
        "y_test = np.concatenate([rul_df.iloc[id].values for id in range(len(rul_df))]).astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4P2tzPt34f3O"
      },
      "outputs": [],
      "source": [
        "rmse, r2, mae = evaluate_model(model, test_seq_array, y_test)\n",
        "print(f'RMSE: {rmse}, R2: {r2}, MAE: {mae}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ep62XV07R0A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgX_SGT_8Nhf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gc1mFEkS7Yuz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Xu9fkcQI7btp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcnoKGNW4e7K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUczn6xi4e2j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHmEYVET4e0F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1atG-G-t4eub"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpzYeWZp4pHJbrKZNTHI8R",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}