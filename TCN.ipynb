{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7onNBzgQeA93hrQDRV/z+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swastik200/30DaysOfDSA/blob/main/TCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HrRDzVrhPMAE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU\n",
        "from tensorflow.keras.callbacks import TensorBoard, LearningRateScheduler\n",
        "import time"
      ],
      "metadata": {
        "id": "urg3roEQPe5l"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjuAicuWPjby",
        "outputId": "9049c153-b129-42d0-b65b-2e311451d018"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/CMAPSS.zip\" \"/content\"\n",
        "!unzip CMAPSS.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRpuOWh-PyfD",
        "outputId": "0f1e04dc-08f2-4fa7-ab40-e47570509947"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  CMAPSS.zip\n",
            "  inflating: CMAPSS/readme.txt       \n",
            "  inflating: CMAPSS/RUL_FD001.txt    \n",
            "  inflating: CMAPSS/RUL_FD002.txt    \n",
            "  inflating: CMAPSS/RUL_FD003.txt    \n",
            "  inflating: CMAPSS/RUL_FD004.txt    \n",
            "  inflating: CMAPSS/test_FD001.txt   \n",
            "  inflating: CMAPSS/test_FD002.txt   \n",
            "  inflating: CMAPSS/test_FD003.txt   \n",
            "  inflating: CMAPSS/test_FD004.txt   \n",
            "  inflating: CMAPSS/train_FD001.txt  \n",
            "  inflating: CMAPSS/train_FD002.txt  \n",
            "  inflating: CMAPSS/train_FD003.txt  \n",
            "  inflating: CMAPSS/train_FD004.txt  \n",
            "  inflating: CMAPSS/x.txt            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/CMAPSS/'\n",
        "col_names = ['unit_number', 'time_cycles', 'setting_1', 'setting_2', 'setting_3'] + ['sensor_{}'.format(i) for i in range(1, 22)]\n",
        "df_train = pd.read_csv(path+'train_FD001.txt', sep='\\s+', header=None, names=col_names)\n",
        "df_test = pd.read_csv(path+'test_FD001.txt', sep='\\s+', header=None, names=col_names)\n",
        "y_test = pd.read_csv(path+'RUL_FD001.txt', sep='\\s+', header=None, names=['RUL'])\n"
      ],
      "metadata": {
        "id": "yJXjZ4AJVg3X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_targets(data_length, early_rul):\n",
        "    early_rul_duration = data_length - early_rul\n",
        "    if early_rul_duration <= 0:\n",
        "        return np.arange(data_length - 1, -1, -1)\n",
        "    else:\n",
        "        new_early_rul = early_rul * np.ones(early_rul_duration)\n",
        "        origin_rul = np.arange(early_rul - 1, -1, -1)\n",
        "        return np.append(new_early_rul, origin_rul)"
      ],
      "metadata": {
        "id": "UJHwRtXmVlmC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def process_input_data_with_targets(input_data, target_data, window_length, shift):\n",
        "    num_batches = int(np.floor((len(input_data) - window_length) / shift)) + 1\n",
        "    num_features = input_data.shape[1]\n",
        "    output = np.repeat(np.nan, repeats=num_batches * window_length * num_features)\n",
        "    output_data = output.reshape(num_batches, window_length, num_features)\n",
        "\n",
        "    if target_data is None:\n",
        "        for batch in range(num_batches):\n",
        "            output_data[batch, :, :] = input_data[(0 + shift * batch):(0 + shift * batch + window_length), :]\n",
        "        return output_data\n",
        "    else:\n",
        "        output_targets = np.repeat(np.nan, repeats=num_batches)\n",
        "        for batch in range(num_batches):\n",
        "            window_start = shift * batch\n",
        "            window_end = window_start + window_length\n",
        "\n",
        "            output_data[batch, :, :] = input_data[window_start:window_end, :]\n",
        "            output_targets[batch] = target_data[window_end - 1]\n",
        "        return output_data, output_targets"
      ],
      "metadata": {
        "id": "LvlYmvFkVpNm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_test_data(test_data_for_an_engine, window_length, shift, num_test_windows=1):\n",
        "    max_num_test_batches = int(np.floor((len(test_data_for_an_engine) - window_length) / shift)) + 1\n",
        "\n",
        "    if max_num_test_batches < num_test_windows:\n",
        "        required_len = (max_num_test_batches - 1) * shift + window_length\n",
        "        batched_test_data_for_an_engine = process_input_data_with_targets(\n",
        "            test_data_for_an_engine[-required_len:, :],\n",
        "            target_data=None,\n",
        "            window_length=window_length,\n",
        "            shift=shift\n",
        "        )\n",
        "        return batched_test_data_for_an_engine, max_num_test_batches\n",
        "    else:\n",
        "        required_len = (num_test_windows - 1) * shift + window_length\n",
        "        batched_test_data_for_an_engine = process_input_data_with_targets(\n",
        "            test_data_for_an_engine[-required_len:, :],\n",
        "            target_data=None,\n",
        "            window_length=window_length,\n",
        "            shift=shift\n",
        "        )\n",
        "        return batched_test_data_for_an_engine, num_test_windows"
      ],
      "metadata": {
        "id": "WtLmqCe5VuP_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_first_column = df_train[\"unit_number\"]\n",
        "test_data_first_column = df_test[\"unit_number\"]"
      ],
      "metadata": {
        "id": "vRcJUpqYVzes"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "\n",
        "train_data = scaler.fit_transform(df_train.drop(columns=['unit_number', 'setting_1', 'setting_2', 'sensor_1', 'sensor_5', 'sensor_6', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']))\n",
        "test_data = scaler.transform(df_test.drop(columns=['unit_number', 'setting_1', 'setting_2', 'sensor_1', 'sensor_5', 'sensor_6', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']))"
      ],
      "metadata": {
        "id": "qZMlpPiJV4yd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.DataFrame(data=np.c_[train_data_first_column, train_data])\n",
        "test_data = pd.DataFrame(data=np.c_[test_data_first_column, test_data])"
      ],
      "metadata": {
        "id": "O8UBrpGEV8sf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_machines = len(train_data[0].unique())\n",
        "num_test_machines = len(test_data[0].unique())\n",
        "window_length = 30\n",
        "shift = 1\n",
        "early_rul = 125\n",
        "num_test_windows = 5\n"
      ],
      "metadata": {
        "id": "d8c52cHBV_zS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_train_data = []\n",
        "processed_train_targets = []\n"
      ],
      "metadata": {
        "id": "ef8FRLYjWHgf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in np.arange(1, num_train_machines + 1):\n",
        "    temp_train_data = train_data[train_data[0] == i].drop(columns=[0]).values\n",
        "    temp_train_targets = process_targets(data_length=temp_train_data.shape[0], early_rul=early_rul)\n",
        "    data_for_a_machine, targets_for_a_machine = process_input_data_with_targets(\n",
        "        temp_train_data, temp_train_targets, window_length=window_length, shift=shift\n",
        "    )\n",
        "    processed_train_data.append(data_for_a_machine)\n",
        "    processed_train_targets.append(targets_for_a_machine)\n",
        "\n",
        "processed_train_data = np.concatenate(processed_train_data)\n",
        "processed_train_targets = np.concatenate(processed_train_targets)\n"
      ],
      "metadata": {
        "id": "3BrE03gdWKIU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_test_data = []\n",
        "num_test_windows_list = []\n",
        "\n",
        "for i in np.arange(1, num_test_machines + 1):\n",
        "    temp_test_data = test_data[test_data[0] == i].drop(columns=[0]).values\n",
        "    test_data_for_an_engine, num_windows = process_test_data(\n",
        "        temp_test_data, window_length=window_length, shift=shift, num_test_windows=num_test_windows\n",
        "    )\n",
        "    processed_test_data.append(test_data_for_an_engine)\n",
        "    num_test_windows_list.append(num_windows)\n",
        "\n",
        "processed_test_data = np.concatenate(processed_test_data)\n",
        "true_rul = y_test.values"
      ],
      "metadata": {
        "id": "JoDj3PffWOIe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = np.random.permutation(len(processed_train_targets))\n",
        "processed_train_data, processed_train_targets = processed_train_data[index], processed_train_targets[index]"
      ],
      "metadata": {
        "id": "73uAfSBVWSvo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_train_data, processed_val_data, processed_train_targets, processed_val_targets = train_test_split(\n",
        "    processed_train_data, processed_train_targets, test_size=0.2, random_state=666\n",
        ")"
      ],
      "metadata": {
        "id": "D91r69F0WbCH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GRU_model():\n",
        "    input_shape = (window_length, 15)\n",
        "    model = Sequential([\n",
        "        GRU(128, input_shape=input_shape, return_sequences=True, activation=\"tanh\"),\n",
        "        GRU(64, activation=\"tanh\", return_sequences=True),\n",
        "        GRU(32, activation=\"tanh\"),\n",
        "        Dense(96, activation=\"relu\"),\n",
        "        Dense(128, activation=\"relu\"),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "eROs80HyWeAx"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scheduler(epoch):\n",
        "    if epoch < 10:\n",
        "        return 0.001\n",
        "    else:\n",
        "        return 0.0001\n",
        "\n",
        "tf_callback = TensorBoard(log_dir=\"./logs\")\n",
        "callback = LearningRateScheduler(scheduler, verbose=0)\n",
        "\n",
        "batch_size = 50\n",
        "epochs = 20"
      ],
      "metadata": {
        "id": "pMPripGyWhM1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.layers import Reshape, Permute, concatenate\n",
        "\n",
        "def TCN_model():\n",
        "    input_shape = (window_length, 15)\n",
        "    kernel_size = 3\n",
        "    num_filters = 64\n",
        "    dilations = [1, 2, 4, 8, 16]\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = inputs  # Fix the indentation\n",
        "    skip_connections = []\n",
        "    for dilation_rate in dilations:\n",
        "        x = Conv1D(num_filters, kernel_size, padding='causal', dilation_rate=dilation_rate, activation='relu')(x)\n",
        "        skip_connections.append(x)\n",
        "\n",
        "    x = concatenate(skip_connections)\n",
        "    x = Conv1D(num_filters, kernel_size=1, padding='same', activation='relu')(x)\n",
        "    x = Conv1D(num_filters, kernel_size=1, padding='same', activation='relu')(x)\n",
        "    x = Conv1D(1, kernel_size=1, padding='same')(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "wzr6OLrhWmsp"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = { \"TCN\": TCN_model()}"
      ],
      "metadata": {
        "id": "w4dkbEj3W_pO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(true_rul ,pred_rul, model):\n",
        "  MAE = mean_absolute_error(true_rul, pred_rul)\n",
        "  RMSE = np.sqrt(mean_squared_error(true_rul, pred_rul))\n",
        "  r2 = r2_score(true_rul, pred_rul)\n",
        "  print(\"Testing : R-square = \",r2,'MAE = ',MAE,\"RMSE = \", RMSE)"
      ],
      "metadata": {
        "id": "Ukaro_YxXD1F"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_train_loss = []\n",
        "history_val_loss = []\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(\"================\", model_name, \"================\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Slice the input data to match the expected shape\n",
        "    processed_train_data_sliced = processed_train_data[:, :, :15]\n",
        "    processed_val_data_sliced = processed_val_data[:, :, :15]\n",
        "\n",
        "    # Compile the model with run_eagerly=True\n",
        "    model.compile(optimizer='adam', loss='mse', run_eagerly=True)\n",
        "\n",
        "    model_history = model.fit(processed_train_data_sliced, processed_train_targets, epochs=epochs,\n",
        "                              validation_data=(processed_val_data_sliced, processed_val_targets),\n",
        "                              callbacks=[tf_callback, callback],\n",
        "                              batch_size=batch_size, verbose=0)\n",
        "    rul_pred = model.predict(processed_test_data[:, :, :15], verbose=0).reshape(-1)\n",
        "    end_time = time.time()\n",
        "\n",
        "    preds_for_each_engine = np.split(rul_pred, np.cumsum(num_test_windows_list)[:-1])\n",
        "    mean_pred_for_each_engine = [np.average(ruls_for_each_engine, weights=np.repeat(1 / num_windows, num_windows))\n",
        "                                  for ruls_for_each_engine, num_windows in zip(preds_for_each_engine,\n",
        "                                                                                  num_test_windows_list)]\n",
        "\n",
        "    print('Training : loss = ', model_history.history['loss'][-1])\n",
        "    print('Validation : loss = ', model_history.history['val_loss'][-1])\n",
        "\n",
        "    # Ensure true_rul and mean_pred_for_each_engine have the same length\n",
        "    true_rul_trimmed = true_rul[:len(mean_pred_for_each_engine)]\n",
        "    evaluate(true_rul_trimmed, mean_pred_for_each_engine, model_name)\n",
        "\n",
        "    history_train_loss.append(model_history.history['loss'])\n",
        "    history_val_loss.append(model_history.history['val_loss'])\n",
        "    print('Run Time :', int(end_time - start_time), 'sec')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEpTxRfEV_1c",
        "outputId": "29b9d813-9db4-449a-e994-fca379a811c5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================ TCN ================\n",
            "Training : loss =  129.2417755126953\n",
            "Validation : loss =  135.8905792236328\n",
            "Testing : R-square =  0.884709985431825 MAE =  10.959075937390331 RMSE =  14.109954689556538\n",
            "Run Time : 842 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oPYzMFA9V_5r"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S9W4PadEfhbm"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YcJViaKnff6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bx83_BBkd7Xx"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gx53uDfAVhBs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}